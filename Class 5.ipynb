{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL1-CG.3135: Real Estate Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Multiple Features and the Valid Application of Regression Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is trivial to extend the single-feature linear model to a linear model that simultaneously incorporates multiple features.  \n",
    "    * The interpretation of the results of a statistical model that uses multiple features is the same the interpretation of the partial derivative from the calculus of many variables: \n",
    "        * the effect of a small change in a particular feature on a label (or outcome).  \n",
    "\n",
    "\n",
    "\n",
    "* A model with $K$ features, $x_{ik}$ and label $y_i$:\n",
    "\n",
    "$y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "\n",
    "\n",
    "* The $K$ features $x_{ik}$ influence the label $y_i$ through the $K$-vector, $\\beta$, which we estimate statistically.\n",
    "    * For example, $y_i$ might be the cap rate on a building, and the $x_{ik}$'s are the characteristics of a building, such as its location, its height or its age.\n",
    "\n",
    "\n",
    "\n",
    "* A specific partial derivative interpretation.\n",
    "\n",
    "${\\displaystyle \\frac{\\partial E(y_i)}{\\partial x_{ik}}=\\beta_k}$\n",
    "\n",
    "\n",
    "\n",
    "* For those interested in ancient history, see the [Frisch–Waugh–Lovell theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem).  \n",
    "\n",
    "\n",
    "\n",
    "* One can applu the basic principle of multivariate calculus: \n",
    "    * Holding everything else constant, what is the impact of a feature on an outcome of interest?\n",
    "\n",
    "\n",
    "\n",
    "* **Bottom line is simple**: Fit the linear model with multiple features.  The basic approach to hypothesis testing remains unchanged.  The challenge is the interpretation of the results and the **valid application** of regression diagnostics. \n",
    "\n",
    "\n",
    "\n",
    "### Application: The Fama-French Factor Models\n",
    "* The development of factor models (the use of momentum indicators) was not driven by finance theory.  \n",
    "\n",
    "\n",
    "\n",
    "* It was driven by behavioral observation: namely that people buy when prices are going up and sell when prices are going down.  \n",
    "    * As a result, Fama and French developed proxies for momentum.  \n",
    "    * **It was an exercise in data mining**, but it allows us to property assess model performance using the famous R$^2$ metric.\n",
    "\n",
    "\n",
    "\n",
    "#### Data Dictionary of the Fama-French Daily Factors\n",
    "* mktrf: market returns minus risk free rate.  **Augmented beta** with standard interpretation.\n",
    "* smb: the average return on the nine small stock portfolios minus the average return on the nine big stock portfolios.  Effect unclear.\n",
    "* hml: the average return on the two value portfolios minus the average return on the two growth portfolios.  Effect unclear.\n",
    "* rmw: the average return on the two robust operating profitability portfolios minus the average return on the two weak operating profitability portfolios.  Effect unclear.\n",
    "* cma: the average return on the two conservative investment portfolios minus the average return on the two aggressive investment portfolios.  Effect unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Digression on Merging Data\n",
    "\n",
    "* One to one: easy given a unique ID to link\n",
    "* One to many (or many to to one): needs a rules-based mechanism to assign (consider investment flows among counterparties)\n",
    "* Many to many: almost impossible (calculus is easy in comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit \n",
    "\n",
    "### The R$^2$ Metric: Its Frequent Abuse in Practice\n",
    "* Different statisical computing environments typically produce the same regression ouput, albeit formatted differently.  \n",
    "    * Portions of this information is used for **regression diagnostics**.  \n",
    "    * We have begun to cover regression coefficients and their interpretation, as well as the use of confidence intervals for classical hypothesis testing. \n",
    "\n",
    "\n",
    "\n",
    "* The R$^2$ *goodness of fit* metric is a frequently-cited regression **diagnostic**.  (Technically, it is not a **statistic** because it is not dependent on only the sample data, as we will see.  Hence, classically it is called a **diagnostic**.)  \n",
    "    * If a linear regression uses a constant (which should be included in practice), the R$^2$ is bounded between 0 and 1.  \n",
    "    * It measures the share of the variation in $y$ explained by the variation in the features, the $x$'s, used in a model.  Given this definition, **bigger is better** is the first place that people go to evaluate the quality of the model, which is unwarranted.\n",
    "    \n",
    "    \n",
    "\n",
    "> \"However, it can still be challenging to determine what is a good R$^2$ value, and in general, this will depend on the application.  For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error.  In this case, we would expect to see an R$^2$ value that is extremely close to 1, and a substantially smaller R$^2$ might indicate serious problems with the experiment in which the data were generated.  On the other hand, in typical application in biology, pyschology, marketing and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large.  In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R$^2$ value well below 0.1 might be more realistic.\" \n",
    "\n",
    "> Trevor Hastie, Robert Tibshirani, et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My R$^2$ Is One!  Only the Best R$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example re-inforces the point above from Professors Hastie and Tibshirani to consider your application (or use case).  In processes that change slowly, the predictive power of a model (or representation) should be very good, reflected in the R$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Diagnostics to Assess Model Quality\n",
    "\n",
    "Adjusted R$^2$: A metric that captures the penalty in the use of a large number of features with little explanatory power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is evidence of \"omitted variable bias\"\n",
    "* Failing to include land_size has biased up the measured effect of unit_size because the two features are positively correlated.\n",
    "* The two features, however, have independent effects on sales prices.\n",
    "* In this example, we can compare the R-squared measures across the two models because one is nested in the other.  \n",
    "* We see that the R-squared rises from 0.302 to 0.387, indicating a considerable improvement in the fit of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More features\n",
    "* Now let's examine what happens when we account for the age of the dwelling at the date of sale.\n",
    "* Thoughts about how age might affect sales prices?\n",
    "* Negative impact: old houses (of equal size) have lower sales prices.\n",
    "* Note, however, that there is little impact on the effects of the other features when we add age to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator Variables: Supervised to Semi-supervised\n",
    "\n",
    "* Indicator variables capture potential 'discontinuities'.\n",
    "* Todt Hill is an upscale area in Staten Island.\n",
    "* Suppose we want to capture this feature of the data, which we can do using the indicator \"todt\". \n",
    "* 3.574e+05 = 357,400 dollar premium in Todt Hill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Non-Linearities to the Linear Model\n",
    "\n",
    "* Suppose we want to consider the potential of non-linearities in $y=f(x)$.\n",
    "* We can use the linear model to do so, but must consider the calculus behind it.\n",
    "* One conjecture is that age has a diminishing effect on sales prices.\n",
    "* How to capture this conjecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Interpret?\n",
    "\n",
    "A model with $K$ features, $x_{ik}$ and label $y_i$: $y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "$\\frac{\\partial{E(y_i})}{\\partial{x_{ik}}}=\\hat{\\beta_k}$\n",
    "\n",
    "$\\frac{\\partial{E(\\text{sales price}})}{\\partial{\\text{age}}}=\\hat{\\beta_\\text{age}} + 2\\cdot\\hat{\\beta_\\text{age2}}\\cdot{\\text{age}} = 997 - 2\\cdot17\\cdot\\text{age} = 997 - 34\\cdot\\text{age}$\n",
    "\n",
    "The average age of a house in our data is 36 years, so $997 - 34*36 = 997 - 1224 = -227$.  Therefore, when a house hits its average age, its value is declining every year.  \n",
    "\n",
    "Indeed, we can solve the simple algebra problem, $997 - 34*age = 0$, for age to obtain $\\text{age}\\approx29$.  For the first 29 years, house prices on average rise for each year, and then begin to decline for subsequent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine these results.  \n",
    "* Prices appear to be linear in unit size.\n",
    "* Prices appear to be quadratic in land_size: a positive but diminishing effect.  At what point does the positive effect disappear.\n",
    "* Prices appear to be quadatric in age: initially rising and then falling in age as houses reach the sample's average age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Elasticities\n",
    "* It may be of use to engineer your features so that results are unit free.\n",
    "* The interpretation is: for at 1% change in the feature, what is the % change in the label.\n",
    "* This can be achieved by tranforming the feature and labels using logarithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine these results.\n",
    "\n",
    "* A 10% change in unit size increases sales price by 3.4%.\n",
    "* A 10% change in land size increases sales price by 2.6%.\n",
    "* A 10% change in age decreases sales prices by 0.5%.\n",
    "* Prices in Todt Hill are about 44% higher holding all else constant.\n",
    "* Is this model directly comparable to the one above?\n",
    "* No because we non-linearly transformed both the label and the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
